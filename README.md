# Multimodal Retrieval Augmented Generation with LLMs

This project harnesses the power of OpenAI's language models (LMs) like GPT-3.5 and GPT-4 for multimodal retrieval and augmented generation. It focuses on analyzing, summarizing, and indexing diverse data types - text, tables, and images. Summaries are stored in a Chroma vectorstore and an InMemoryStore, using OpenAIEmbeddings for sophisticated indexing. The system is designed for advanced information synthesis across formats, priming it for future integrations with multimodal LLMs, including GPT4-V and CLIP, to revolutionize AI-driven content processing and creation.

## Approaches to add images to RAG
![image](https://github.com/piyushgit011/multimodal_rag_gpt4v/assets/96625965/66cdfe20-91b4-4feb-958c-3ed4e4c013c0)

## Stack for tables, figures
![image](https://github.com/piyushgit011/multimodal_rag_gpt4v/assets/96625965/bcb6dfc9-5180-41c3-8ed4-da34c3d5f48a)
